{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-18T09:52:39.425632Z",
     "start_time": "2025-05-18T09:52:35.716513Z"
    }
   },
   "source": [
    "#### FUNCTION TO CREATE VOCAB\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Set, Tuple\n",
    "\n",
    "import spacy\n",
    "\n",
    "from utils.io import load_json, save_json\n",
    "from utils.text_processing import get_lemmas_flat_set, get_lemma_word\n",
    "\n",
    "%cd /home/alessandro/work/repo/task-testbed\n",
    "%pwd\n",
    "\n",
    "remove_stopwords = True\n",
    "join_composed_kw = False\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "\n",
    "def find_vocabulary_for_label(examples: List, target_label: int) -> Set[str]:\n",
    "    keyword_set = set()\n",
    "    for example in examples:\n",
    "        label = example[\"label\"]\n",
    "        text = example[\"text\"]\n",
    "        # texts_train.add(text)\n",
    "        if target_label == label == 1:\n",
    "            lw, w = get_lemma_word(example['w_p'], nlp, remove_stopwords, join_result=join_composed_kw)\n",
    "            la, a = get_lemma_word(example['w_a'], nlp, remove_stopwords, join_result=join_composed_kw)\n",
    "            lcws, cws = get_lemmas_flat_set(example['c_w'], nlp, remove_stopwords,\n",
    "                                            join_composed_expressions=join_composed_kw)\n",
    "\n",
    "            keyword_set.update(la)\n",
    "            keyword_set.update(a)\n",
    "            keyword_set.update(lw)\n",
    "            keyword_set.update(w)\n",
    "            [keyword_set.add(c) for c in lcws]\n",
    "            [keyword_set.add(c) for c in cws]\n",
    "        elif target_label == label == 0:\n",
    "            keywords_example = set()\n",
    "            for text_word in nlp(text.lower()):\n",
    "                if text_word.text.strip() and not text_word.is_punct and (\n",
    "                        not remove_stopwords or not text_word.is_stop):\n",
    "                    keywords_example.add(text_word.lemma_)\n",
    "                    keywords_example.add(text_word.text.strip())\n",
    "            keyword_set.update(keywords_example)\n",
    "    return keyword_set"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alessandro/work/repo/task-testbed/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alessandro/work/repo/task-testbed\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T09:52:50.632295Z",
     "start_time": "2025-05-18T09:52:42.225529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### CREATE VOCAB WITH TRAINING KEYWORDS (CONTEXT + PUN WORD + ALT WORD)\n",
    "\n",
    "NEW_DATASET = \"data/legacy/puns/puneval_split\"\n",
    "\n",
    "train_examples = load_json(Path(NEW_DATASET) / \"train_puns_original.json\")\n",
    "test_examples = load_json(Path(NEW_DATASET) / \"test_puns.json\")\n",
    "print(len(train_examples), len(test_examples))\n",
    "\n",
    "train_kws = find_vocabulary_for_label(train_examples, 1)\n",
    "# train_kws_0 = find_vocabulary_for_label(train_examples, 0)\n",
    "print(len(train_kws))"
   ],
   "id": "e282e2d87ba8a06f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1248 1341\n",
      "2274\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T10:06:06.538436Z",
     "start_time": "2025-05-18T10:06:06.300933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Union\n",
    "##### FIND TOKENS IN TRAINING SET THAT HAVE HIGH CORRELATION WITH POSITIVE OR NEGATIVE EXAMPLES IN THE TEST\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from typing import Optional, List\n",
    "from tqdm import tqdm\n",
    "\n",
    "TOP_N_KEYWORDS = 20\n",
    "\n",
    "\n",
    "def find_high_correlation_tokens(train_examples, test_examples, top_k_coeffs: float, ngram_range: Tuple[int, int],\n",
    "                                 min_df: Union[float, int], max_features: int = None,\n",
    "                                 stop_words=None,\n",
    "                                 vocabulary: Optional[List[str]] = None, min_diff: int = 1) -> Tuple[\n",
    "    pd.DataFrame, float]:\n",
    "    # Step 1: Create a binary feature matrix for the training examples\n",
    "    vectorizer = CountVectorizer(vocabulary=vocabulary, binary=True, ngram_range=ngram_range, analyzer=\"word\",\n",
    "                                 min_df=min_df, max_features=max_features, stop_words=stop_words)\n",
    "    X_train = vectorizer.fit_transform([example[\"text\"] for example in train_examples]).toarray()\n",
    "    y_train = np.array([example[\"label\"] for example in train_examples])\n",
    "\n",
    "    # discover how the ngrams works\n",
    "\n",
    "    # Step 2: Train a Logistic Regression model\n",
    "    model = LogisticRegression(solver=\"liblinear\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    if vocabulary is None:\n",
    "        vocabulary = list(sorted(vectorizer.vocabulary_.keys()))\n",
    "\n",
    "    # Step 3: Analyze feature coefficients\n",
    "    coefficients = model.coef_[0]\n",
    "\n",
    "    # Step 4: Create a binary feature matrix for the test examples\n",
    "    X_test = vectorizer.transform([example[\"text\"] for example in test_examples]).toarray()\n",
    "    y_test = np.array([example[\"label\"] for example in test_examples])\n",
    "    acc = model.score(X_test, y_test)\n",
    "\n",
    "    # Step 5: Count keyword occurrences in the test set\n",
    "    total_count = X_test.sum(axis=0)\n",
    "    # Count occurrences of keywords in positive and negative examples\n",
    "    pos_counts = X_test[y_test == 1].sum(axis=0)\n",
    "    neg_counts = X_test[y_test == 0].sum(axis=0)\n",
    "\n",
    "    # Create DataFrame for counts\n",
    "    combined_df = pd.DataFrame({\n",
    "        'Keyword': vocabulary,\n",
    "        'Coefficient': coefficients,\n",
    "        'Support': total_count,\n",
    "        'Positive Count': pos_counts,\n",
    "        'Negative Count': neg_counts\n",
    "    })\n",
    "\n",
    "    # Sort the combined DataFrame by coefficient\n",
    "    combined_df = combined_df.sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "    # DEBUG STUFF\n",
    "    # exising_words = [vocabulary[jj] for jj in np.nonzero(total_count)[0].tolist()]\n",
    "    # elem = [test_examples[jj][\"text\"] for jj in np.nonzero(X_test[:, vocabulary.index(\"add oil\")])[0].tolist()]\n",
    "    combined_df['Diff'] = (combined_df['Positive Count'] - combined_df['Negative Count'])\n",
    "    combined_df = combined_df[combined_df['Support'] >= min_diff]\n",
    "\n",
    "    # Calculate the threshold for the top-k%\n",
    "    threshold = combined_df['Coefficient'].quantile(1 - top_k_coeffs)\n",
    "\n",
    "    # Filter the DataFrame to keep only rows with coefficients in the top-k%\n",
    "    combined_df = combined_df[combined_df['Coefficient'] >= threshold]\n",
    "\n",
    "    # Display the top keywords by positive coefficients along with their counts\n",
    "    # print(\"Top keywords by positive coefficients:\")\n",
    "    # print(combined_df[combined_df['Coefficient'] > 0].head(20))\n",
    "\n",
    "    # print(\"\\nTop keywords by negative coefficients:\")\n",
    "    # print(combined_df[combined_df['Coefficient'] < 0].head(20))\n",
    "\n",
    "    # Display keywords that are present in all positive examples\n",
    "    # print(\"\\nKeywords present in all positive examples:\")\n",
    "    # print(combined_df[combined_df['Positive Count'] == (y_test == 1).sum()])\n",
    "\n",
    "    combined_df['Ratio'] = (combined_df['Positive Count'] - combined_df['Negative Count']).abs() / combined_df[\n",
    "        ['Positive Count', 'Negative Count']].max(axis=1)\n",
    "    # print(combined_df['Ratio'].min(), combined_df['Ratio'].max())\n",
    "    # Calculate the combined score (you can adjust the formula as needed)\n",
    "    combined_df['Combined Score'] = combined_df['Coefficient'].abs() * combined_df['Ratio']\n",
    "\n",
    "    return combined_df, acc\n",
    "\n",
    "\n",
    "top_train_kw, acc = find_high_correlation_tokens(train_examples, test_examples, 0.1, ngram_range=(1, 1), min_df=10,\n",
    "                                                 vocabulary=list(train_kws), stop_words=\"english\", min_diff=10)\n",
    "print(f\"Selected {len(top_train_kw)} keywords. Acc={acc}.\")\n",
    "print()\n",
    "top_train_unigrams, acc = find_high_correlation_tokens(train_examples, test_examples, 0.1, ngram_range=(1, 1),\n",
    "                                                       min_df=10,\n",
    "                                                       stop_words=\"english\", min_diff=10)\n",
    "print(f\"Selected {len(top_train_unigrams)} uni-grams. Acc={acc}.\")\n",
    "print()\n",
    "top_train_bigrams, acc = find_high_correlation_tokens(train_examples, test_examples, 0.2, ngram_range=(2, 2), min_df=3,\n",
    "                                                      min_diff=3)\n",
    "print(f\"Selected {len(top_train_bigrams)} bi-grams. Acc={acc}.\")\n",
    "print()\n",
    "top_train_trigrams, acc = find_high_correlation_tokens(train_examples, test_examples, 0.3, ngram_range=(3, 3), min_df=3,\n",
    "                                                       min_diff=3)\n",
    "print(f\"Selected {len(top_train_trigrams)} tri-grams. Acc={acc}.\")\n",
    "print()\n",
    "top_train_4grams, acc = find_high_correlation_tokens(train_examples, test_examples, 0.3, ngram_range=(4, 4), min_df=3,\n",
    "                                                     min_diff=3)\n",
    "print(f\"Selected {len(top_train_4grams)} 4-grams. Acc={acc}.\")\n",
    "top_ngrams = pd.concat([top_train_kw, top_train_bigrams, top_train_trigrams, top_train_4grams], ignore_index=True)\n",
    "# top_ngrams = top_train_kw\n",
    "# Remove rows with duplicate 'Keyword' values, keeping the first occurrence\n",
    "top_ngrams = top_ngrams.sort_values(by='Combined Score', ascending=False)\n",
    "top_ngrams = top_ngrams.drop_duplicates(subset='Keyword', keep='first').nlargest(TOP_N_KEYWORDS, 'Combined Score')\n",
    "# Select the top-k n-grams based on the combined score\n",
    "top_k_ngrams = top_ngrams\n",
    "print(f\"Selected {len(top_k_ngrams)} n-grams\")\n",
    "\n",
    "tot_g = 0\n",
    "words = list()\n",
    "for i in tqdm(range(len(top_k_ngrams))):\n",
    "    row = top_k_ngrams.iloc[i]\n",
    "    g = int(row[\"Diff\"])\n",
    "    if g > 0:\n",
    "        words.append({\"expression\": str(row[\"Keyword\"]), \"count\": g})\n",
    "        tot_g += g\n",
    "save_json(Path(\"dataset_scripts/shortcuts_v2.json\"), words)\n",
    "print(tot_g)\n",
    "\n",
    "top_ngrams"
   ],
   "id": "181c14bc690f30e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 5 keywords. Acc=0.6823266219239373.\n",
      "\n",
      "Selected 4 uni-grams. Acc=0.5734526472781506.\n",
      "\n",
      "Selected 30 bi-grams. Acc=0.6964951528709918.\n",
      "\n",
      "Selected 5 tri-grams. Acc=0.5831469052945563.\n",
      "\n",
      "Selected 2 4-grams. Acc=0.5764354958985831.\n",
      "Selected 20 n-grams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 18047.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                Keyword  Coefficient  Support  Positive Count  Negative Count  \\\n",
       "40  never die they just     2.779405       58              58               0   \n",
       "0                   tom     2.617024       61              61               0   \n",
       "5              said tom     2.527773       24              24               0   \n",
       "6                it was     1.819988       23              23               0   \n",
       "35       never die they     1.818570       60              60               0   \n",
       "7                he was     1.744455       18              18               0   \n",
       "1                   old     1.798727       75              70               5   \n",
       "36        die they just     1.604793       58              58               0   \n",
       "37         she was only     1.570773       19              19               0   \n",
       "2                  said     1.703012       50              46               4   \n",
       "4              daughter     1.552084       20              19               1   \n",
       "9             never die     1.459171       62              62               0   \n",
       "10               lot of     1.359050        8               8               0   \n",
       "8              when the     1.509240       18              16               2   \n",
       "11              she was     1.320019       22              22               0   \n",
       "12             die they     1.313349       60              60               0   \n",
       "15            they just     1.205771       60              60               0   \n",
       "16         because they     1.174770        7               7               0   \n",
       "17               but he     1.161906        5               5               0   \n",
       "13              have to     1.283814       21              19               2   \n",
       "\n",
       "    Diff     Ratio  Combined Score  \n",
       "40    58  1.000000        2.779405  \n",
       "0     61  1.000000        2.617024  \n",
       "5     24  1.000000        2.527773  \n",
       "6     23  1.000000        1.819988  \n",
       "35    60  1.000000        1.818570  \n",
       "7     18  1.000000        1.744455  \n",
       "1     65  0.928571        1.670247  \n",
       "36    58  1.000000        1.604793  \n",
       "37    19  1.000000        1.570773  \n",
       "2     42  0.913043        1.554924  \n",
       "4     18  0.947368        1.470396  \n",
       "9     62  1.000000        1.459171  \n",
       "10     8  1.000000        1.359050  \n",
       "8     14  0.875000        1.320585  \n",
       "11    22  1.000000        1.320019  \n",
       "12    60  1.000000        1.313349  \n",
       "15    60  1.000000        1.205771  \n",
       "16     7  1.000000        1.174770  \n",
       "17     5  1.000000        1.161906  \n",
       "13    17  0.894737        1.148676  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Coefficient</th>\n",
       "      <th>Support</th>\n",
       "      <th>Positive Count</th>\n",
       "      <th>Negative Count</th>\n",
       "      <th>Diff</th>\n",
       "      <th>Ratio</th>\n",
       "      <th>Combined Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>never die they just</td>\n",
       "      <td>2.779405</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.779405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tom</td>\n",
       "      <td>2.617024</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.617024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>said tom</td>\n",
       "      <td>2.527773</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.527773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>it was</td>\n",
       "      <td>1.819988</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.819988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>never die they</td>\n",
       "      <td>1.818570</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.818570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>he was</td>\n",
       "      <td>1.744455</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.744455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>old</td>\n",
       "      <td>1.798727</td>\n",
       "      <td>75</td>\n",
       "      <td>70</td>\n",
       "      <td>5</td>\n",
       "      <td>65</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>1.670247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>die they just</td>\n",
       "      <td>1.604793</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.604793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>she was only</td>\n",
       "      <td>1.570773</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.570773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>said</td>\n",
       "      <td>1.703012</td>\n",
       "      <td>50</td>\n",
       "      <td>46</td>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>1.554924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>daughter</td>\n",
       "      <td>1.552084</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>1.470396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>never die</td>\n",
       "      <td>1.459171</td>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.459171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>lot of</td>\n",
       "      <td>1.359050</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.359050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>when the</td>\n",
       "      <td>1.509240</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.320585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>she was</td>\n",
       "      <td>1.320019</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.320019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>die they</td>\n",
       "      <td>1.313349</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.313349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>they just</td>\n",
       "      <td>1.205771</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.205771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>because they</td>\n",
       "      <td>1.174770</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.174770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>but he</td>\n",
       "      <td>1.161906</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.161906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>have to</td>\n",
       "      <td>1.283814</td>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>1.148676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
