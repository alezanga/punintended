{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.io import load_json, save_json\n",
    "from utils.text_processing import get_lemma_word\n",
    "\n",
    "%cd /home/alessandro/coding/PycharmProjects/task-testbed\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "REM_SW = False\n",
    "\n",
    "# check how many conforming examples we have\n",
    "structures = load_json(Path(\"dataset_generation/structures.json\"))\n",
    "\n",
    "test_set = load_json(Path(\"data/public/puneval/test.json\"))\n",
    "\n",
    "for structure, structure_template in tqdm(structures.items()):\n",
    "    # if structure not in [\"daughter\", \"never_die\", \"used\"]:\n",
    "    #     continue\n",
    "    structure_data = load_json(Path(f\"data/public/punny_pattern/{structure}.json\"))\n",
    "    assert len({e[\"id\"] for e in structure_data}) == len(structure_data), \"Duplicated IDs\"\n",
    "\n",
    "    lemma_set, word_set = get_lemma_word(structure_template.replace(\"[X]\", \"\").strip(), nlp, REM_SW)\n",
    "\n",
    "    # Step 3: Create a set of distinct words\n",
    "    bad_examples = list()\n",
    "    good_examples_n = 0\n",
    "    for example in structure_data:\n",
    "        text = example[\"text\"]\n",
    "        # if example[\"label\"] == 1:\n",
    "        lemma_words_set, example_words_set = get_lemma_word(text, nlp, REM_SW)\n",
    "        if not set(word_set).issubset(set(example_words_set)):\n",
    "            bad_examples.append(text)\n",
    "        else:\n",
    "            good_examples_n += 1\n",
    "    print(f\"Good examples for '{structure}': \", good_examples_n)\n",
    "    print(f\"Bad examples for '{structure}': \", len(bad_examples))\n",
    "\n",
    "    out_path = Path(f\"dataset_generation/analysis/bad_{structure}.json\")\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    save_json(out_path, bad_examples)\n",
    "\n",
    "    test_puns_matching_template = set()\n",
    "    for test_example in test_set:\n",
    "        text = test_example[\"text\"]\n",
    "        # if test_example[\"label\"] == 1:\n",
    "        lemma_words_set, example_words_set = get_lemma_word(text, nlp, True)\n",
    "        if set(lemma_words_set).issubset(set(lemma_set)):\n",
    "            test_puns_matching_template.add(test_example[\"id\"])\n",
    "\n",
    "    structure_data_ids = {e[\"id\"] for e in structure_data}\n",
    "    missing_ids = test_puns_matching_template - structure_data_ids\n",
    "    if missing_ids:\n",
    "        print(f\"Missing TEST puns NOT in '{structure}' set: {len(missing_ids)}\")\n",
    "        save_json(out_path.parent / f\"missing_{structure}.json\", list(missing_ids))\n"
   ],
   "id": "e19d1949aca98a13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# SCRIPT TO DETECT DUPLICATES, JUST TO BE USED AS WORKBENCH, NO OUTPUT ON FILE\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "%cd /home/alessandro/coding/PycharmProjects/task-testbed\n",
    "\n",
    "def find_duplicate_sentences(sentences, threshold=0.8):\n",
    "    # Create a TF-IDF Vectorizer\n",
    "    vectorizer = TfidfVectorizer().fit_transform(sentences)\n",
    "    vectors = vectorizer.toarray()\n",
    "\n",
    "    # Compute cosine similarity matrix\n",
    "    cosine_sim = cosine_similarity(vectors)\n",
    "\n",
    "    # Find duplicates\n",
    "    duplicates = set()\n",
    "    for i in range(len(cosine_sim)):\n",
    "        for j in range(i + 1, len(cosine_sim)):\n",
    "            if cosine_sim[i][j] > threshold:  # Check if similarity is above the threshold\n",
    "                duplicates.add((i, j))\n",
    "\n",
    "    return duplicates\n",
    "\n",
    "\n",
    "def read_paragraphs_from_file(file_path):\n",
    "    paragraphs = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Strip whitespace and check if the line is not empty\n",
    "            line = line.strip()\n",
    "            if line:  # Only add non-empty lines\n",
    "                paragraphs.append(line)\n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from utils.io import load_json\n",
    "\n",
    "# Example usage\n",
    "file_path = Path(\"data/public/punny_pattern/tom.json\")\n",
    "paragraphs = load_json(file_path)\n",
    "\n",
    "all_examples = [e[\"text\"] for e in paragraphs]\n",
    "\n",
    "duplicates = find_duplicate_sentences(all_examples, threshold=0.6)\n",
    "\n",
    "# Print results\n",
    "for i, j in duplicates:\n",
    "    print(f\"Duplicate sentences found: \\n1: {paragraphs[i]} \\n2: {paragraphs[j]}\\n\")"
   ],
   "id": "a718e84c2aeb16f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# THIS WILL REMOVE PUNS, so handle with care\n",
    "# THIS WILL REMOVE PUNS, so handle with care\n",
    "# THIS WILL REMOVE PUNS, so handle with care\n",
    "# THIS WILL REMOVE PUNS, so handle with care\n",
    "# THIS WILL REMOVE PUNS, so handle with care\n",
    "\n",
    "from utils.io import load_json, save_json\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from utils.text_processing import get_lemma_word\n",
    "from tqdm import tqdm\n",
    "\n",
    "%cd /home/alessandro/coding/PycharmProjects/task-testbed\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "REM_SW = False\n",
    "\n",
    "# check how many conforming examples we have\n",
    "structures = load_json(Path(\"dataset_generation/structures.json\"))\n",
    "\n",
    "for structure, structure_template in tqdm(structures.items()):\n",
    "    structure_data = load_json(Path(f\"data/public/punny_pattern/{structure}.json\"))\n",
    "\n",
    "    lemma_set, word_set = get_lemma_word(structure_template.replace(\"[X]\", \"\").strip(), nlp, REM_SW)\n",
    "\n",
    "    # Step 3: Create a set of distinct words\n",
    "    bad_examples = list()\n",
    "    good_examples = list()\n",
    "    for example in structure_data:\n",
    "        text = example[\"text\"]\n",
    "        # if example[\"label\"] == 1:\n",
    "        lemma_words_set, example_words_set = get_lemma_word(text, nlp, REM_SW)\n",
    "        if not set(word_set).issubset(set(example_words_set)):\n",
    "            bad_examples.append(text)\n",
    "        else:\n",
    "            good_examples.append(example)\n",
    "    print(f\"Good examples for '{structure}': \", len(good_examples))\n",
    "    print(f\"Bad examples for '{structure}': \", len(bad_examples))\n",
    "\n",
    "    save_json(Path(f\"data/public/punny_pattern/{structure}.json\"), good_examples)"
   ],
   "id": "c153991ecc8178e5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
